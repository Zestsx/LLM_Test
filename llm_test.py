# -*- coding: utf-8 -*-
"""LLM_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vxwC0FY-zlMhfc-Nqn10kYOF1hd-vGY1
"""

#!pip install langchain openai streamlit streamlit_chat chromadb==0.3.26

#!pip install constants

# Imports
import streamlit as st
import os
from streamlit_chat import message

# import
from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.document_loaders import CSVLoader
from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI

os.environ["OPENAI_API_KEY"] = "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"

# load the document and split it into chunks
loader = CSVLoader(file_path='/content/drive/My Drive/Colab/data.txt')
documents = loader.load()

# Set API keys and the models to use
model_id = "gpt-3.5-turbo"

# Define the LLM we plan to use. Here we are going to use ChatGPT 3.5 turbo
llm=ChatOpenAI(model_name = model_id, temperature=0.2)


#------------------------------------------------------------------
# Setup streamlit app

# Display the page title and the text box for the user to ask the question
st.title('âœ¨ Query your Documents ')
prompt = st.text_input("Enter your question to query your Financial Data ")

# Display the current response. No chat history is maintained

if prompt:
    # stuff chain type sends all the relevant text chunks from the document to LLM
    response = index.query(llm=llm, question = prompt, chain_type = 'stuff')

    # Write the results from the LLM to the UI
    st.write("<br><i>" + response + "</i><hr>", unsafe_allow_html=True )